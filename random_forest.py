# using sklearn instead of PyTorch because PyTorch doesn't have a decision tree class

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import root_mean_squared_error
import pandas as pd
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt


df = pd.read_csv("housing.csv")

x = df.drop(columns=["median_house_value", "ocean_proximity"])  # drop y and categorical column
y = df['median_house_value']    # no double brackets because sklearn expects (N,) instead of (N, 1)

x = x.fillna(x.mean())  # fill in missing values

x = x.to_numpy()
y = y.to_numpy()


# train_test_split does shuffling for you and decides the test and training split
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.3, random_state=42
)

# instantiating the model
forest = RandomForestRegressor(
    max_depth=3,
    min_samples_leaf=50,
    random_state=42
)

# training
forest.fit(x_train, y_train)

# predicting on test set
y_hat = forest.predict(x_test)

# test rmse
test_rmse = root_mean_squared_error(y_test, y_hat)
# print("RMSE:", test_rmse)

# predicting on training set
y_hat_train = forest.predict(x_train)

# train rmse
train_rmse = root_mean_squared_error(y_train, y_hat_train)

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)

# finetuning depth
# for depth in [2, 4, 6, 8, 10]:
#     forest = RandomForestRegressor(
#         max_depth=depth,
#         min_samples_leaf=50,
#         random_state=42
#     )
#     forest.fit(x_train, y_train)

#     train_rmse = root_mean_squared_error(y_train, forest.predict(x_train))
#     test_rmse  = root_mean_squared_error(y_test, forest.predict(x_test))

    # print(f"depth={depth} | train={train_rmse:.0f} | test={test_rmse:.0f}")

# finetuning min_samples_leaf
# for leaf in [5, 20, 50, 100, 300]:
#     forest = RandomForestRegressor(
#         max_depth=6,
#         min_samples_leaf=leaf,
#         random_state=42
#     )
#     forest.fit(x_train, y_train)

#     train_rmse = root_mean_squared_error(y_train, forest.predict(x_train))
#     test_rmse  = root_mean_squared_error(y_test, forest.predict(x_test))

    # print(f"leaf={leaf} | train={train_rmse:.0f} | test={test_rmse:.0f}")


# finetuned forest
forest = RandomForestRegressor(
    max_depth=10,
    min_samples_leaf=5,
    random_state=42
)

forest.fit(x_train, y_train)

train_rmse = root_mean_squared_error(y_train, forest.predict(x_train))
test_rmse  = root_mean_squared_error(y_test, forest.predict(x_test))

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)

# in the end, test rmse reduced by 26k and training rmse reduced by almost 36k
# there does seem to be some variance/overfitting because the test set has a 10k difference from the training set,
# but I think this can be attributed to my hyperparameters, and also I didn't touch the 
# n_estimators parameter, which controls how many trees there are in the forest

# visualizing the forest
plt.figure(figsize=(32, 14))
plot_tree(
    forest.estimators_[0],  # first tree in the forest (0-99)
    feature_names=df.drop(columns=["median_house_value", "ocean_proximity"]).columns,
    filled=True,
    fontsize=10
)
plt.show()

# Random forest is called this because it has randomness in feature selection, as every tree within the forest
# uses only some features, and the dataset for each tree is generated by using bootstrapping, where each dataset
# is sampled from the original with replacement, and then the final prediction comes from taking the average
# of each tree output (at least in regression, for classification, you would use majority voting), 
# and this is called aggregating, and together, this process is called bagging - bootstrapping + aggregating.
# Lastly, the forest comes from the fact that the model is made up of multiple trees. 

# Also, these trees try to be as uncorrelated as possible, otherwise every tree would be the same, and there
# would be no point of having multiple trees.

# Random forest is used in the first place to solve the problem of a single decision tree having too much variance
